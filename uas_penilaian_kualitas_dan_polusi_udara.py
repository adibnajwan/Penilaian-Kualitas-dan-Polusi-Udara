# -*- coding: utf-8 -*-
"""UAS_Penilaian Kualitas dan Polusi Udara

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11xTvpx12-9EN7o2QjNKTNjApYEGwVsZ9

# Dataset Overview
"""

# prompt: import dataset
!pip install dataset
import dataset

import pandas as pd

# Load the dataset
file_path = '/content/updated_pollution_dataset.csv'
data = pd.read_csv(file_path)

"""#  1. Pre-Processing dan EDA

## Pre-Processing Dataset

### Memeriksa tipe data
"""

# Step 0: Memeriksa tipe data
print("\nData Types of Each Column:")
data.dtypes

"""### Mengganti nama kolom"""

# Step 1: mengganti nama kolom
data.rename(columns=lambda x: x.strip().replace(' ', '_').lower(), inplace=True)
data.info()

"""### Memeriksa nilai NULL"""

# Step 2: memeriksa nilai null
null_values = data.isnull().sum()
print("Null Values in Each Column:")
print(null_values)

"""### Mengubah tipe data"""

# Step 3: mengubah tipe data
data['air_quality'] = data['air_quality'].astype('category')
data.dtypes

"""### Menampilkan Summary"""

# Step 4: menampilkan summary
summary_statistics = data.describe(include='all')
print("\nSummary Statistics:")
print(summary_statistics)

"""### Menampilkan matriks korelasinya"""

#menampilkan matriks korelasinya
numerical_data = data.select_dtypes(include=['number']) # Select only numerical columns
correlation_matrix = numerical_data.corr()
print("\nCorrelation Matrix:")
print(correlation_matrix)

from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import mutual_info_classif

le=LabelEncoder()
data['air_quality']=le.fit_transform(data['air_quality'])
data.info()

fig,ax=plt.subplots(3,3,figsize=(30,15))
ax=ax.flatten()
i=0

for col in data.columns:
    if col!='air_quality' and i<9:
        sns.boxplot(data=data,x=col,ax=ax[i])
        i+=1

plt.tight_layout()
plt.show()

for col in data.drop(columns='air_quality').columns:
    q1=np.quantile(data[col],0.25)
    q3=np.quantile(data[col],0.75)
    iqr=q3-q1
    lb=q1-iqr*1.5
    ub=q3+iqr*1.5
    data=data[(data[col]>=lb)&(data[col]<=ub)]

data.info()

tdata=data.copy()
x=tdata.drop(columns='air_quality')
y=tdata['air_quality']
mi=mutual_info_classif(x,y)
mi_data=pd.DataFrame({'Feature':x.columns,'Mutual Information':mi})
mi_data=mi_data.sort_values(by='Mutual Information', ascending=False).reset_index(drop=True)

# Visualizing Mutual Information and correlation
fig,ax=plt.subplots(2,1,figsize=(15,20))
sns.heatmap(data.corr(),annot=True,cmap='magma',ax=ax[0])
ax[0].set_title('Correlation')
sns.barplot(x='Mutual Information',y='Feature', data=mi_data,ax=ax[1])
ax[1].set_title('Mutual Information')
plt.tight_layout()
plt.show()

"""## Exploratory Data Analysis (EDA)"""

import matplotlib.pyplot as plt
import seaborn as sns

# 1. Bar Chart: Distribution of PM2.5 Concentration Levels
plt.figure(figsize=(8, 5))
data['pm2.5'].plot(kind='hist', bins=20, color='dodgerblue', edgecolor='black')
plt.title('Distribution of PM2.5 Concentration Levels')
plt.xlabel('PM2.5 Concentration (µg/m³)')
plt.ylabel('Frequency')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# 2. Pie Chart: Population Density Categories
population_density_bins = pd.cut(data['population_density'], bins=[0, 500, 1000, 5000, 10000], labels=['Low', 'Medium', 'High', 'Very High'])
plt.figure(figsize=(6, 6))
population_density_bins.value_counts().plot(kind='pie', autopct='%1.1f%%', colors=['#ff9999','#66b3ff','#99ff99','#ffcc99'])
plt.title('Population Density Distribution')
plt.ylabel('')
plt.show()

# 3. Scatter Plot: PM10 vs. Proximity to Industrial Areas
plt.figure(figsize=(8, 5))
sns.scatterplot(x='proximity_to_industrial_areas', y='pm10', data=data, alpha=0.6, color='purple')
plt.title('PM10 Levels vs. Proximity to Industrial Areas')
plt.xlabel('Proximity to Industrial Areas (km)')
plt.ylabel('PM10 Concentration (µg/m³)')
plt.grid(True)
plt.show()

# 4. Box Plot: Temperature by CO Concentration Levels
plt.figure(figsize=(8, 5))
sns.boxplot(x=pd.cut(data['co'], bins=[0, 1, 5, 10], labels=['Low', 'Moderate', 'High']),
            y='temperature', data=data, palette='coolwarm')
plt.title('Temperature Distribution by CO Concentration Levels')
plt.xlabel('CO Concentration Category')
plt.ylabel('Temperature (°C)')
plt.xticks(rotation=45)
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

fig,ax=plt.subplots(3,3,figsize=(20,20))
ax=ax.flatten()
i=0

for col in data.columns:
    if(col!='air_quality'):
        sns.histplot(data=data,x=col,kde=True,ax=ax[i])
        i+=1
plt.tight_layout()

fig,ax=plt.subplots(1,2,figsize=(20,5))

tdata=data['air_quality'].value_counts().reset_index()
ax[0].pie(tdata['count'],labels=tdata[col],autopct='%.2f%%')
ax[0].set_aspect('equal')
sns.countplot(data=data,x=col,ax=ax[1])
plt.tight_layout()
plt.show()

fig, ax = plt.subplots(3, 3, figsize=(20, 20))
ax = ax.flatten()
i = 0
cols=data.drop(columns='air_quality')
for col in cols:
    sns.boxplot(data=data, x='air_quality', y=col, ax=ax[i])
    i += 1
plt.tight_layout()
plt.show()

fig, ax = plt.subplots(3, 3, figsize=(20, 20))
ax = ax.flatten()
i = 0
cols=data.drop(columns='air_quality')
for col in cols:
    sns.scatterplot(data=data, x='air_quality', y=col, ax=ax[i],hue='air_quality',alpha=0.5)
    i += 1
plt.tight_layout()
plt.show()

plt.figure(figsize=(15,7))
sns.scatterplot(data=data,x='proximity_to_industrial_areas',y='population_density',hue='air_quality',alpha=0.6)
plt.show()

fig,ax=plt.subplots(3,1,figsize=(15,8))
sns.scatterplot(data=data,y='no2',x='proximity_to_industrial_areas',alpha=0.4,ax=ax[0],hue='air_quality')
sns.scatterplot(data=data,y='so2',x='proximity_to_industrial_areas',alpha=0.4,ax=ax[1],hue='air_quality')
sns.scatterplot(data=data,y='co',x='proximity_to_industrial_areas',alpha=0.4,ax=ax[2],hue='air_quality')
plt.tight_layout()
plt.show()

fig,ax=plt.subplots(3,1,figsize=(15,8))
sns.scatterplot(data=data,y='no2',x='population_density',alpha=0.4,ax=ax[0],hue='air_quality')
sns.scatterplot(data=data,y='so2',x='population_density',alpha=0.4,ax=ax[1],hue='air_quality')
sns.scatterplot(data=data,y='co',x='population_density',alpha=0.4,ax=ax[2],hue='air_quality')
plt.tight_layout()
plt.show()

"""# 2. Modeling

## a. Model Machine Learning
- Random Forest Classifier
- Gradient Boosting Classifier
- Ada Boost Classifier
- Decision Tree Classifier
"""

# importing
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV

x=data.drop("air_quality",axis=1)
y=data["air_quality"]

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier

classifier={
    "Random Forest Classifier":RandomForestClassifier(),
    "Gradient Boosting Classifier": GradientBoostingClassifier(),
    "Ada Boost Classifier":AdaBoostClassifier(),
    "Decision Tree Classifier":DecisionTreeClassifier()
}

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix

for name,clf in classifier.items():
    clf.fit(x_train,y_train)
    y_pred=clf.predict(x_test)
    accuracy=accuracy_score(y_test,y_pred)
    print(f"{name}")
    print(f"{accuracy*100}")
    print(classification_report(y_test,y_pred))
    print("-"*60)

accuracy_results={}
for name,clf in classifier.items():
    clf.fit(x_train,y_train)
    y_pred=clf.predict(x_test)
    accuracy=accuracy_score(y_test,y_pred)
    accuracy_results[name]=accuracy
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=np.unique(y), yticklabels=np.unique(y))
    plt.title(f'Confusion Matrix for {name}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

accuracy_df=pd.DataFrame(accuracy_results.items(),columns=["Classifier","Accuracy"])
plt.figure(figsize=(10, 8))
sns.barplot(x="Accuracy", y="Classifier", data=accuracy_df)
plt.title("Accuracy of Different Classifiers")
plt.xlabel("Accuracy")
plt.ylabel("Classifier")
plt.xlim(0, 1)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.show()

"""## b. Hyperparameter Tuning"""

# Hyperparameter Tuning for Random Forest
rf_param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf_random_search = RandomizedSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_distributions=rf_param_grid,
    n_iter=10,  # Number of parameter settings sampled
    scoring='accuracy',
    cv=5,
    verbose=1,
    random_state=42,
    n_jobs=-1  # Use all available CPU cores
)

rf_random_search.fit(x_train, y_train)
print("Best hyperparameters for Random Forest:", rf_random_search.best_params_)
best_rf_model = rf_random_search.best_estimator_
rf_tuned_pred = best_rf_model.predict(x_test)
rf_tuned_accuracy = accuracy_score(y_test, rf_tuned_pred)
print(f"Tuned Random Forest Accuracy: {rf_tuned_accuracy}")

# Hyperparameter Tuning for Gradient Boosting
gb_param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 1],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 0.9, 1.0]
}

gb_random_search = RandomizedSearchCV(
    estimator=GradientBoostingClassifier(random_state=42),
    param_distributions=gb_param_grid,
    n_iter=10,
    scoring='accuracy',
    cv=5,
    verbose=1,
    random_state=42,
    n_jobs=-1
)

gb_random_search.fit(x_train, y_train)
print("Best hyperparameters for Gradient Boosting:", gb_random_search.best_params_)

best_gb_model = gb_random_search.best_estimator_
gb_tuned_pred = best_gb_model.predict(x_test)
gb_tuned_accuracy = accuracy_score(y_test, gb_tuned_pred)
print(f"Tuned Gradient Boosting Accuracy: {gb_tuned_accuracy}")

# Compare and Determine Best Model
print(f"Random Forest Tuned Accuracy: {rf_tuned_accuracy}")
print(f"Gradient Boosting Tuned Accuracy: {gb_tuned_accuracy}")

if rf_tuned_accuracy > gb_tuned_accuracy:
    best_model = best_rf_model
    print("Random Forest is the best model.")
else:
    best_model = best_gb_model
    print("Gradient Boosting is the best model.")

"""## c. Karakteristik model terbaik"""

import numpy as np

# Dengan asumsi 'best_model' dan data relevan didefinisikan dari kode sebelumnya

# Pentingnya Fitur dari model terbaik
if isinstance(best_model, RandomForestClassifier):
    importances = best_model.feature_importances_
    feature_names = x.columns
    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

    print("\nFeature Importance (Random Forest):")
    print(feature_importance_df)

    # Visualize Feature Importance
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
    plt.title('Feature Importance from Random Forest')
    plt.show()

elif isinstance(best_model, GradientBoostingClassifier):
    importances = best_model.feature_importances_
    feature_names = x.columns
    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

    print("\nFeature Importance (Gradient Boosting):")
    print(feature_importance_df)

    # Visualize Feature Importance
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
    plt.title('Feature Importance from Gradient Boosting')
    plt.show()

# Correlation Analysis with Target Variable
correlation_with_target = data.corr()['air_quality'].drop('air_quality')
print("\nCorrelation with Target Variable:")
print(correlation_with_target)

# Analyze relationships between important features and the target
# (Example for 'proximity_to_industrial_areas')
plt.figure(figsize=(8, 6))
sns.scatterplot(x='proximity_to_industrial_areas', y='air_quality', data=data, hue='air_quality', alpha=0.6)
plt.title('Air Quality vs. Proximity to Industrial Areas')
plt.show()

"""Analisis lebih lanjut berdasarkan model yang dipilih:<br>
1. Analisis skor kepentingan fitur. Fitur dengan skor kepentingan yang lebih tinggi berkontribusi lebih banyak pada prediksi model.
2. Bandingkan kepentingan fitur dengan korelasi. Apakah fitur yang sangat penting juga sangat berkorelasi dengan target?
3. Selidiki hubungan antara fitur dan target menggunakan visualisasi dan uji statistik. Apakah hubungan ini sesuai dengan harapan Anda?
4. Pertimbangkan interaksi antara fitur. Apakah efek dari satu fitur bergantung pada nilai fitur lainnya?
5. Jelajahi analisis residual jika berlaku untuk model yang Anda pilih untuk memahami pola dalam kesalahan.

Contoh interpretasi:
- Jika 'proximity_to_industrial_areas' memiliki kepentingan tinggi tetapi korelasi rendah, ini menunjukkan hubungan yang lebih kompleks.
- Visualisasi membantu memahami hubungan; mungkin ada efek non-linier atau ambang batas.
"""